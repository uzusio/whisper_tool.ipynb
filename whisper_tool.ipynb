{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 音声文字起こしツール for whisper\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## 概要\n",
        "OpenAIの[whisper](https://github.com/openai/whisper)を使用した音声文字起こしツールです。\n",
        "Google Colabの無料環境で利用可能です。\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## 使い方\n",
        "\n",
        "\n",
        "1.   編集->ノートブックの設定->ハードウェア アクセラレータ から「GPU」を選択してください（※初回以降は不要です）\n",
        "2.   Googleドライブに「whisper」というフォルダを作成してください（※初回以降は不要です）\n",
        "3.   一番目のセルを実行して、Googleドライブをマウントしてください\n",
        "4.   二番目のセルに設定を記述して、実行してください\n",
        "    1.   DIR_PATH: 文字起こしをする音声ファイルを配置するディレクトリです。文字起こしされたテキストファイルも同ディレクトリに出力されます。デフォルトではドライブ上の「whisper」フォルダになります。\n",
        "    2.   INPUT_FILE: 文字起こしする音声ファイルのファイル名です。 \n",
        "    3.   MODEL_TYPE： 文字起こしの精度を選びます。tiny<base<small<medium<largeの順に精度が上がります。※ダウンロードするモデルのファイルサイズも大きくなります。メモリエラーが起きたらモデルを１段階小さくしてください。\n",
        "    4.   srt_output: 字幕ファイル生成に対応しました。出力ファイルを字幕ファイル形式にしたい場合はチェックを入れてください。\n",
        "5.以降のセルを最後まで実行してください。ランタイム->以降のセルを実行でもOKです。\n",
        "\n",
        "### おまけ\n",
        "マイクからの入力機能も用意してみました。\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "77hp4SaVOw39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## Googleドライブのマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_edxIOabN2AO",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 初期設定\n",
        "#@markdown ファイルを配置するディレクトリを指定\n",
        "DIR_PATH=\"/content/drive/MyDrive/whisper\" #@param {type:\"string\"}\n",
        "#@markdown 入力ファイル名\n",
        "INPUT_FILE=\"\" #@param {type:\"string\"}\n",
        "#@markdown 利用するモデルサイズを選択\n",
        "MODEL_TYPE=\"large\" #@param [\"tiny\", \"base\",\"small\",\"medium\",\"large\"] {type:\"string\"}\n",
        "#@markdown 字幕ファイル(.srt)を出力したい場合下にチェック\n",
        "srt_output=True #@param {type: \"boolean\"}"
      ],
      "metadata": {
        "id": "DCGevpCmrXQB",
        "cellView": "form"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 必要ライブラリのインストール\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install srt"
      ],
      "metadata": {
        "id": "OeCjEqh51ogd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## モデルのロード\n",
        "import whisper\n",
        "\n",
        "model = whisper.load_model(MODEL_TYPE)\n",
        "\n",
        "#@title ## 文字起こし実行\n",
        "file_path = DIR_PATH + \"/\" + INPUT_FILE\n",
        "\n",
        "# 推論\n",
        "result = model.transcribe(file_path)\n"
      ],
      "metadata": {
        "id": "l5iwYvQ6q99Q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 結果をGoogleドライブに保存\n",
        "from datetime import timedelta\n",
        "from srt import Subtitle\n",
        "import srt\n",
        "\n",
        "def add_line(s):\n",
        "    new_s = s\n",
        "    s_count = len(s)\n",
        "    s_max_count = 15\n",
        "    if s_count >= s_max_count:\n",
        "        if (s_count - s_max_count) >= 3:\n",
        "            # 15文字以上、かつ、2行目が3文字以上あれば、改行する\n",
        "            # つまり、18文字以上であれば、15文字で改行する\n",
        "            new_s = s[:s_max_count] + \"\\n\" + s[s_max_count:]\n",
        " \n",
        "    return new_s\n",
        "\n",
        "if srt_output:\n",
        "  subs = []\n",
        "\n",
        "  for data in result[\"segments\"]:\n",
        "    index = data[\"id\"] + 1\n",
        "    start = data[\"start\"]\n",
        "    end = data[\"end\"]\n",
        "    text = add_line(data[\"text\"])\n",
        "    sub = Subtitle(index=1, start=timedelta(seconds=timedelta(seconds=start).seconds,\n",
        "                                            microseconds=timedelta(seconds=start).microseconds),\n",
        "                   end=timedelta(seconds=timedelta(seconds=end).seconds,\n",
        "                                 microseconds=timedelta(seconds=end).microseconds), content=text, proprietary='')\n",
        " \n",
        "    subs.append(sub)\n",
        "\n",
        "  with open(f\"{file_path}.srt\", \"w\") as f:\n",
        "    f.write(srt.compose(subs))\n",
        "\n",
        "else:\n",
        "  with open(f\"{file_path}.txt\", \"w\") as f:\n",
        "    f.write(f\"▼ Transcription of {INPUT_FILE}\\n\")\n",
        "    for segment in result[\"segments\"]:\n",
        "      f.write(str(segment[\"id\"]) + \" \" + segment[\"text\"] + \"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "2bgXU2Za5b_s"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 結果をテーブル表示で見る\n",
        "import pandas as pd\n",
        "pd.DataFrame(result[\"segments\"])[[\"start\", \"end\", \"text\"]]"
      ],
      "metadata": {
        "id": "u6XT71CM1unn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# マイクから入力機能"
      ],
      "metadata": {
        "id": "7S7Gny2Bdan0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 必要ライブラリのインストール\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install gradio"
      ],
      "metadata": {
        "cellView": "form",
        "id": "UaDf4kfrb91n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## マイクから入力\n",
        "from transformers import pipeline\n",
        "import whisper\n",
        "import gradio as gr\n",
        "\n",
        "if not model:\n",
        "  model = whisper.load_model(MODEL_TYPE)\n",
        "\n",
        "p = pipeline(\"automatic-speech-recognition\")\n",
        "\n",
        "def transcribe(audio):\n",
        "    text = model.transcribe(audio)\n",
        "    return text\n",
        "\n",
        "gr.Interface(\n",
        "    fn=transcribe, \n",
        "    inputs=gr.Audio(source=\"microphone\", type=\"filepath\"), \n",
        "    outputs=\"text\").launch()\n"
      ],
      "metadata": {
        "id": "FIpislkVYNc1",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}